{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src='Sharif_logo.png' width=250 height=250> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Applied Data Science<br>\n",
    "<font color=2565AE size=5>\n",
    "Spring 2025<br>\n",
    "<font color=3C99D size=5>\n",
    "HW5 - Accuracy Metrics <br>\n",
    "<font color=696880 size=4>\n",
    "Ali Mohammadzade Shabestari - 401106482 - Computer Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:29:15.826016Z",
     "start_time": "2025-03-16T19:29:15.819324Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import Binarizer, KBinsDiscretizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, hamming_loss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we make use of `load_diabetes` dataset from `Scikit-Learn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:29:16.469904Z",
     "start_time": "2025-03-16T19:29:15.832033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.002592  0.019907 -0.017646   151.0  \n",
       "1 -0.039493 -0.068332 -0.092204    75.0  \n",
       "2 -0.002592  0.002861 -0.025930   141.0  \n",
       "3  0.034309  0.022688 -0.009362   206.0  \n",
       "4 -0.002592 -0.031988 -0.046641   135.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "\n",
    "# Add target variable\n",
    "df['target'] = diabetes.target\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 1 Regression Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, it is essential to evaluate the performance of a regression model on unseen data. To achieve this, the dataset is typically split into training and testing sets. The training set is used to train the model, while the testing set is used to evaluate its performance. This process helps in assessing how well the model generalizes to new data.\n",
    "\n",
    "The `train_test_split` function from the `sklearn.model_selection` module is commonly used to split the dataset into training and testing sets. It randomly divides the data into specified proportions, ensuring that the model is trained and tested on different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:29:16.495093Z",
     "start_time": "2025-03-16T19:29:16.488324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing and feature selection, we can proceed with training a regression model. Here, we use the `LinearRegression` model from `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 2 Regression Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the performance of a regression model, several metrics can be used to measure how well the model's predictions match the actual values. Here are some commonly used metrics:\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "Mean Squared Error (MSE) is the average of the squared differences between the predicted and actual values. It is calculated as:\n",
    "\n",
    "$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "where \\( y_i \\) is the actual value, \\( \\hat{y}_i \\) is the predicted value, and \\( n \\) is the number of observations. MSE gives a higher weight to larger errors, making it sensitive to outliers.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error (MAE) is the average of the absolute differences between the predicted and actual values. It is calculated as:\n",
    "\n",
    "$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "MAE provides a linear score that does not emphasize large errors as much as MSE.\n",
    "\n",
    "### Mean Absolute Percentage Error (MAPE)\n",
    "Mean Absolute Percentage Error (MAPE) is the average of the absolute percentage differences between the predicted and actual values. It is calculated as:\n",
    "\n",
    "$\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|$\n",
    "\n",
    "MAPE expresses the error as a percentage, making it easier to interpret in the context of the data.\n",
    "\n",
    "### R-squared (R²)\n",
    "R-squared (R²) is a statistical measure that represents the proportion of the variance for the dependent variable that is explained by the independent variables in the model. It is calculated as:\n",
    "\n",
    "$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "\n",
    "where \\( \\bar{y} \\) is the mean of the actual values. R² ranges from 0 to 1, with higher values indicating a better fit of the model to the data.\n",
    "\n",
    "Each of these metrics provides different insights into the performance of a regression model, and they are often used together to get a comprehensive understanding of the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Metrics:\n",
      "MSE: 2900.194\n",
      "MAE: 42.794\n",
      "MAPE: 0.375\n",
      "R2 Score: 0.453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regression Metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Regression Metrics:\\nMSE: {mse:.3f}\\nMAE: {mae:.3f}\\nMAPE: {mape:.3f}\\nR2 Score: {r2:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>MSE:</b> The model’s predictions deviate significantly from actual values on average.\n",
    "\n",
    "- <b>MAE:</b> On average, the model’s predictions are off by about 42.79 units, which might be large depending on the scale of the target variable.\n",
    "\n",
    "- <b>MAPE:</b> The model’s predictions are, on average, 37.5% off from the actual values, which is relatively high.\n",
    "\n",
    "- <b>R2:</b> The model explains only 45.3% of the variance, meaning that more than half of the variability in the target variable remains unexplained. This suggests that the model may be underfitting or missing important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 1 Binary Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a regression problem, binary classification can be applied by converting the continuous target variable into a binary variable. This is typically done by defining a threshold value, above which the target variable is classified as one class (e.g., 1) and below which it is classified as the other class (e.g., 0).\n",
    "\n",
    "For example, in the diabetes dataset, we could convert the continuous target variable (which represents a quantitative measure of disease progression) into a binary variable indicating whether the progression is above or below a certain threshold. This allows us to apply binary classification techniques to predict whether a patient's disease progression is high or low.\n",
    "\n",
    "Binary classification accuracy metrics, such as precision, recall, F1-score, and AUC-ROC, can then be used to evaluate the performance of the classification model. These metrics provide insights into the model's ability to correctly classify instances into the two classes, which is particularly useful when dealing with imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target into binary: High (>=140) vs Low (<140)\n",
    "y_binary = (df['target'] >= 140).astype(int)\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(df.drop(columns=['target']), y_binary, test_size=0.2, random_state=42)\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_bin, y_train_bin)\n",
    "y_pred_bin = classifier.predict(X_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 2 Binary Classification Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: \"Of all the instances that were predicted as positive, how many were actually positive?\" Precision is calculated as:\n",
    "\n",
    "$\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}$\n",
    "\n",
    "### Recall\n",
    "Recall (also known as Sensitivity or True Positive Rate) is the ratio of correctly predicted positive observations to all the observations in the actual class. It answers the question: \"Of all the instances that were actually positive, how many were correctly predicted as positive?\" Recall is calculated as:\n",
    "\n",
    "$\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}$\n",
    "\n",
    "### F1-Score\n",
    "The F1-Score is the weighted average of Precision and Recall. It is especially useful when you need a balance between Precision and Recall, and when you have an uneven class distribution. The F1-Score is calculated as:\n",
    "\n",
    "$\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "These metrics provide different insights into the performance of a classification model, and they are often used together to get a comprehensive understanding of the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification Metrics:\n",
      "Precision: 0.732\n",
      "Recall: 0.714\n",
      "F1-Score: 0.723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision_bin = precision_score(y_test_bin, y_pred_bin)\n",
    "recall_bin = recall_score(y_test_bin, y_pred_bin)\n",
    "f1_bin = f1_score(y_test_bin, y_pred_bin)\n",
    "\n",
    "print(f\"Binary Classification Metrics:\\nPrecision: {precision_bin:.3f}\\nRecall: {recall_bin:.3f}\\nF1-Score: {f1_bin:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Precision:</b> Out of all the instances classified as positive, 73.2% were correctly identified. A lower precision indicates a high false positive rate, meaning the model often misclassifies negatives as positives.\n",
    "\n",
    "- <b>Recall:</b> The model correctly identified 71.4% of actual positive cases. A lower recall suggests a high false negative rate, meaning the model is missing some actual positives.\n",
    "\n",
    "- <b>F1:</b> Since F1-score = 72.3%, it suggests a good trade-off between precision and recall. If precision and recall differ significantly, F1-score is more informative than using either metric alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 1 Multi-class Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a regression problem, multiclass classification can be applied by converting the continuous target variable into multiple discrete classes. This is typically done by defining multiple threshold values, which divide the target variable into several classes. For example, in the diabetes dataset, we could convert the continuous target variable (which represents a quantitative measure of disease progression) into multiple classes indicating different levels of progression (e.g., low, medium, high).\n",
    "\n",
    "By applying multiclass classification techniques, we can gain a more nuanced understanding of the model's performance and its ability to distinguish between different levels of the target variable. This approach is especially valuable in scenarios where the target variable exhibits a wide range of values and a simple binary classification would not capture the complexity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing target into 3 categories (Low, Medium, High)\n",
    "encoder = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "y_multi = encoder.fit_transform(df[['target']]).astype(int).flatten()\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(df.drop(columns=['target']), y_multi, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier_multi = LogisticRegression(max_iter=1000)\n",
    "classifier_multi.fit(X_train_multi, y_train_multi)\n",
    "y_pred_multi = classifier_multi.predict(X_test_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# 5. 2 Multi-class Classification Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision for Each Class\n",
    "Precision for each class is the ratio of correctly predicted instances of that class to the total instances predicted as that class. It answers the question: \"Of all the instances predicted as a specific class, how many were actually of that class?\" High precision indicates a low false positive rate.\n",
    "\n",
    "### Recall for Each Class\n",
    "Recall for each class is the ratio of correctly predicted instances of that class to the total actual instances of that class. It answers the question: \"Of all the instances that actually belong to a specific class, how many were correctly predicted?\" High recall indicates a low false negative rate.\n",
    "\n",
    "### Macro-Averaged F1-Score\n",
    "Macro-averaged F1-Score calculates the F1-Score for each class independently and then takes the average. This treats all classes equally, regardless of their frequency. It is useful when you want to evaluate the model's performance across all classes without considering class imbalance.\n",
    "\n",
    "### Weighted-Averaged F1-Score\n",
    "Weighted-averaged F1-Score calculates the F1-Score for each class independently and then takes the average, weighted by the number of true instances for each class. This accounts for class imbalance by giving more importance to classes with more instances.\n",
    "\n",
    "### Micro-Averaged F1-Score\n",
    "Micro-averaged F1-Score aggregates the contributions of all classes to compute the average F1-Score. It calculates the total true positives, false negatives, and false positives across all classes and then computes the F1-Score. This method is useful when you want to evaluate the overall performance of the model, especially in the presence of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.642     0.956     0.768        45\n",
      "           1      0.545     0.375     0.444        32\n",
      "           2      0.000     0.000     0.000        12\n",
      "\n",
      "    accuracy                          0.618        89\n",
      "   macro avg      0.396     0.444     0.404        89\n",
      "weighted avg      0.521     0.618     0.548        89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Multi-class Classification Report:\")\n",
    "print(classification_report(y_test_multi, y_pred_multi, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tClass 2 is completely ignored → Try balancing the dataset or adjusting class weights.\n",
    "\n",
    "•\tClass 1 has poor recall → Consider feature selection or a different model.\n",
    "\n",
    "•\tMajority class (0) dominates predictions → A weighted loss function or oversampling may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Football Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Loss\n",
    "\n",
    "Hamming Loss is a metric used to evaluate the performance of a multi-label classification model. It calculates the fraction of labels that are incorrectly predicted. In other words, it measures the proportion of labels that are incorrectly predicted across all samples.\n",
    "\n",
    "The formula for Hamming Loss is:\n",
    "\n",
    "$$\n",
    "\\text{Hamming Loss} = \\frac{1}{n_{\\text{samples}} \\times n_{\\text{labels}}} \\sum_{i=1}^{n_{\\text{samples}}} \\sum_{j=1}^{n_{\\text{labels}}} \\mathbf{1}(y_{ij} \\neq \\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n_{\\text{samples}}$ is the number of samples.\n",
    "\n",
    "- $n_{\\text{labels}}$ is the number of labels.\n",
    "\n",
    "- $y_{ij}$ is the true value of the $j$-th label for the $i$-th sample.\n",
    "\n",
    "- $\\hat{y}_{ij}$ is the predicted value of the $j$-th label for the $i$-th sample.\n",
    "\n",
    "- $\\mathbf{1}$ is the indicator function that returns 1 if $y_{ij} \\neq \\hat{y}_{ij}$ and 0 otherwise.\n",
    "\n",
    "A lower Hamming Loss indicates better performance, as it means fewer labels are incorrectly predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_classes = 4\n",
    "np.random.seed(42)\n",
    "y_true_multi_label = np.random.randint(0, 2, size=(n_samples, n_classes))\n",
    "y_pred_multi_label = np.random.randint(0, 2, size=(n_samples, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Label Classification (Football Example):\n",
      "Hamming Loss: 0.499\n"
     ]
    }
   ],
   "source": [
    "# Multi-label Classification Metric (Hamming Loss)\n",
    "hamming = hamming_loss(y_true_multi_label, y_pred_multi_label)\n",
    "print(f\"\\nMulti-Label Classification (Football Example):\\nHamming Loss: {hamming:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming Loss of 0.499 indicates that 49.9% of the labels are incorrectly predicted.\n",
    "\n",
    "Since this is a multi-label problem, the model’s predictions are incorrect nearly half the time for each label across all samples. This is relatively high, which suggests that the model might not be capturing the relationships well or is struggling to predict the correct labels for each player."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
