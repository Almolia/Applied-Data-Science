{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src='Sharif_logo.png' width=250 height=250> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Applied Data Science<br>\n",
    "<font color=2565AE size=5>\n",
    "Spring 2025<br>\n",
    "<font color=3C99D size=5>\n",
    "HW6 - Regression Methods <br>\n",
    "<font color=696880 size=4>\n",
    "Ali Mohammadzade Shabestari - 401106482 - Computer Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:29:15.826016Z",
     "start_time": "2025-03-16T19:29:15.819324Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load & Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Concrete_Data.xls')\n",
    "\n",
    "X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize X\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1. 1. R2 Score > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R2 Score: 0.6275416055429022\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Linear Regression R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve a better R² score, we can use polynomial features to capture the non-linear relationships in the data. By transforming the input features into higher-degree polynomial terms, the model can better fit the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (degree = 2) R2 Score: 0.784268504972648\n"
     ]
    }
   ],
   "source": [
    "# Create polynomial features of degree 2\n",
    "X_poly2 = PolynomialFeatures(degree=2)\n",
    "X_train_poly2 = X_poly2.fit_transform(X_train)\n",
    "X_test_poly2 = X_poly2.transform(X_test)\n",
    "scaler = StandardScaler()\n",
    "X_train_poly2 = scaler.fit_transform(X_train_poly2)\n",
    "X_test_poly2 = scaler.transform(X_test_poly2)\n",
    "\n",
    "# Linear Regression with Polynomial Features\n",
    "poly_regressor = LinearRegression()\n",
    "poly_regressor.fit(X_train_poly2, y_train)\n",
    "y_pred = poly_regressor.predict(X_test_poly2)\n",
    "r2_poly = r2_score(y_test, y_pred)\n",
    "print(\"Polynomial Regression (degree = 2) R2 Score:\", r2_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1. 2. R2 Score > 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create polynomial features of dgree 3 or higher, to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (degree = 3)  R2 Score: 0.8437173906891395\n"
     ]
    }
   ],
   "source": [
    "X_poly3 = PolynomialFeatures(degree=3)\n",
    "X_train_poly3 = X_poly3.fit_transform(X_train)\n",
    "X_test_poly3 = X_poly3.transform(X_test)\n",
    "scaler = StandardScaler()\n",
    "X_train_poly3 = scaler.fit_transform(X_train_poly3)\n",
    "X_test_poly3 = scaler.transform(X_test_poly3)\n",
    "# Linear Regression with Polynomial Features\n",
    "poly_regressor = LinearRegression()\n",
    "poly_regressor.fit(X_train_poly3, y_train)\n",
    "y_pred = poly_regressor.predict(X_test_poly3)\n",
    "r2_poly = r2_score(y_test, y_pred)\n",
    "print(\"Polynomial Regression (degree = 3)  R2 Score:\", r2_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 2. Kernel Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel regression is a non-parametric technique used to model complex relationships between variables. It leverages kernel functions to map input data into a higher-dimensional space, enabling the model to capture non-linear patterns.\n",
    "\n",
    " Common kernel functions include Radial Basis Function (RBF), polynomial, and linear kernels. In Python, `KernelRidge` from `sklearn.kernel_ridge` can be used for kernel regression. Key parameters include `alpha` (regularization strength) and `kernel` (type of kernel). The model is trained using the `fit()` method and predictions are made using the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 4.897926237984521\n"
     ]
    }
   ],
   "source": [
    "# Perform Kernel Regression\n",
    "kernel_ridge = KernelRidge(kernel='rbf', alpha=0.5)\n",
    "kernel_ridge.fit(X_train_scaled, y_train)\n",
    "y_pred_kernel_ridge = kernel_ridge.predict(X_test_scaled)\n",
    "\n",
    "# Caculate Mean Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred_kernel_ridge)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression R2 Score: 0.5145631067961165\n"
     ]
    }
   ],
   "source": [
    "# First generate the categorical labels\n",
    "y_train_labels = pd.qcut(y_train, q=2, labels=['low', 'high'])\n",
    "y_test_labels = pd.qcut(y_test, q=2, labels=['low', 'high'])\n",
    "\n",
    "# Then encode them\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['low', 'high'])  # or just fit on y_train_labels\n",
    "y_train_multi = label_encoder.transform(y_train_labels)\n",
    "y_test_multi = label_encoder.transform(y_test_labels)\n",
    "\n",
    "# Perform Logistic Regression\n",
    "logistic_regressor = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logistic_regressor.fit(X_train_scaled, y_train_multi)\n",
    "y_pred_multi = logistic_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate r2 score\n",
    "r2_logistic = r2_score(y_test_multi, y_pred_multi)\n",
    "print(\"Logistic Regression R2 Score:\", r2_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umm, let's perform it on Polynomial Features (degree = 5). We expect to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression R2 Score: 0.5339805825242718\n",
      "Logistic Regression Accuracy: 0.883495145631068\n"
     ]
    }
   ],
   "source": [
    "# Perform Logistic Regression\n",
    "logistic_regressor = LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "logistic_regressor.fit(X_train_poly3, y_train_multi)\n",
    "y_pred_multi = logistic_regressor.predict(X_test_poly3)\n",
    "\n",
    "# Calculate r2 score\n",
    "r2_logistic = r2_score(y_test_multi, y_pred_multi)\n",
    "print(\"Logistic Regression R2 Score:\", r2_logistic)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test_multi, y_pred_multi)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the improvment is good but not enough. Itried higher degrees, but they didn't make a big effect. (Maximum R2 was observed at degree = 5)\n",
    "\n",
    "I guess since the structure of data is not designed for a classification problem, reaching a higher R2 Score is not a must. Just look at accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge Regression minimizes the following cost function:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^{n} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 + \\alpha \\|\\mathbf{w}\\|^2_2\n",
    "$$\n",
    "\n",
    "where \n",
    "- `w` is the weight vector (model parameters)  \n",
    "- `xᵢ` is the input features for instance *i*  \n",
    "- `yᵢ` is the true value for instance *i*  \n",
    "- `α ≥ 0` is the regularization strength (hyperparameter)  \n",
    "- `‖w‖²₂ = ∑ wⱼ²` is the L2 norm (sum of squared coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R2 Score: 0.6275416891423036\n"
     ]
    }
   ],
   "source": [
    "# Perform Ridge Regression with new parameters\n",
    "ridge_regressor = Ridge(alpha=0.5)  \n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_regressor.predict(X_test)\n",
    "\n",
    "# Calculate R2 Score\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(\"Ridge Regression R2 Score:\", r2_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Polynomial Features of degree = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R2 Score: 0.8411685341628778\n"
     ]
    }
   ],
   "source": [
    "ridge_regressor.fit(X_train_poly3, y_train)\n",
    "y_pred_ridge = ridge_regressor.predict(X_test_poly3)\n",
    "\n",
    "# Calculate R2 Score\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(\"Ridge Regression R2 Score:\", r2_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 5. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression minimizes the following cost function:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^{n} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 + \\alpha \\|\\mathbf{w}\\|_1\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- `w` is the weight vector (model parameters)  \n",
    "- `xᵢ` is the input features for instance *i*  \n",
    "- `yᵢ` is the true value for instance *i*  \n",
    "- `α ≥ 0` is the regularization strength (hyperparameter)  \n",
    "- `‖w‖₁ = ∑ |wⱼ|` is the L1 norm (sum of absolute values of coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression R2 Score: 0.627594030943751\n"
     ]
    }
   ],
   "source": [
    "# Perform Lasso Regression\n",
    "lasso_regressor = Lasso(alpha=0.3)  # You can adjust the alpha parameter\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Calculate R2 Score\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(\"Lasso Regression R2 Score:\", r2_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Polynomial Features of degree = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression R2 Score: 0.7884916793037768\n"
     ]
    }
   ],
   "source": [
    "lasso_regressor.fit(X_train_poly3, y_train)\n",
    "y_pred_lasso = lasso_regressor.predict(X_test_poly3)\n",
    "\n",
    "# Calculate R2 Score\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(\"Lasso Regression R2 Score:\", r2_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 6. How Kernel method helps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick is a method used in machine learning (especially in models like Support Vector Machines and kernelized regression) to implicitly map input data into a higher-dimensional feature space without actually computing the transformation. Instead, it uses a kernel function (like RBF or polynomial) that calculates the inner product of data points in that higher-dimensional space directly.\n",
    "\n",
    "In standard linear regression, if the data is not linearly related to the target, performance suffers. But by using the kernel trick in Kernel Ridge Regression or SVR (Support Vector Regression), we can capture complex, nonlinear relationships in the data—leading to much better fits and predictions without explicitly increasing the model complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
